They are easy to make, but difficult to interpret. For behind every comparison lies an inherent confidence in one side of the equation. This confidence could be in the accuracy, quality, or even a widely accepted opinion. It is this very confidence that provides us a platform to base our interpretation of the comparison of the other side of the equation.

But what if that confidence is misplaced? What if the model we believe in is flawed? What if the measurement we are comparing it to is inaccurate? In that case, the comparison becomes trifling, meaningless.

Since I began working in the solar industry, I have performed several model-to-model and model-to-measurement comparisons. And usually, each of these have been a journey from excitment to frustration. A model-to-model comparison needs me to believe more in one model over the other. And our belief in the 'better' model should rise from a good model-to-measured comparison. But then, high quality measurements are difficult to obtain, especially over a long enough period. 

Instead of shock or surprise, we should expect challenges. Instead of exuberance, we should be skeptical when comparisons workout perfectly. Errors often cancel out and give out a false picture, behind which the danger lies.
